"""æµå¼å¯¹è¯ - ä½¿ç”¨ LangChain ReAct Agent

v2.0: é›†æˆ SkillManager åŠ¨æ€å·¥å…·è·¯ç”±
"""

import json
from typing import Any, AsyncGenerator, Dict, List, Optional

from langchain_core.messages import HumanMessage, SystemMessage, AIMessageChunk, ToolMessage
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langgraph.prebuilt import create_react_agent

from .config import get_config
from .excel_loader import get_loader
from .knowledge_base import get_knowledge_base, format_knowledge_context
from .tools import ALL_TOOLS
from .skill_loader import get_skill_loader


# å½“å‰ä½¿ç”¨çš„æ¨¡å‹åç§°ï¼ˆç”¨äºå®¹é”™åˆ‡æ¢åçš„æ˜¾ç¤ºï¼‰
_current_model: Optional[str] = None


class CustomJSONEncoder(json.JSONEncoder):
    """è‡ªå®šä¹‰ JSON ç¼–ç å™¨ï¼Œå¤„ç† Pandas/Numpy ç±»å‹"""

    def default(self, obj):
        # å¤„ç† Pandas Timestamp
        if hasattr(obj, 'isoformat'):
            return obj.isoformat()
        # å¤„ç† numpy ç±»å‹
        if hasattr(obj, 'item'):
            return obj.item()
        # å¤„ç† numpy æ•°ç»„
        if hasattr(obj, 'tolist'):
            return obj.tolist()
        # å¤„ç† pandas NaT
        if str(obj) == 'NaT':
            return None
        # å¤„ç† pandas NA
        if str(obj) == '<NA>':
            return None
        return super().default(obj)


def json_dumps(obj, **kwargs):
    """ä½¿ç”¨è‡ªå®šä¹‰ç¼–ç å™¨çš„ JSON åºåˆ—åŒ–å‡½æ•°"""
    return json.dumps(obj, cls=CustomJSONEncoder, **kwargs)


SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ Excel æ•°æ®åˆ†æåŠ©æ‰‹ã€‚

## å½“å‰ Excel ä¿¡æ¯
{excel_summary}

## ç›¸å…³çŸ¥è¯†å‚è€ƒ
{knowledge_context}

## å½“å‰å¯ç”¨æŠ€èƒ½
{skills_context}

## å·¥ä½œåŸåˆ™
1. æ ¹æ®ç”¨æˆ·é—®é¢˜ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦ä½¿ç”¨å·¥å…·
2. å¦‚éœ€å·¥å…·ï¼Œè°ƒç”¨åˆé€‚çš„å·¥å…·è·å–æ•°æ®
3. å·¥å…·è°ƒç”¨æˆåŠŸåï¼Œæ ¹æ®ç»“æœå›ç­”ç”¨æˆ·é—®é¢˜
4. **æœ€ç»ˆå›ç­”ç›´æ¥ç»™å‡ºç»“è®ºå’Œåˆ†æ**ï¼Œä¸è¦æè¿°"æˆ‘ä½¿ç”¨äº†xxå·¥å…·"æˆ–"æˆ‘è¿›è¡Œäº†xxæ“ä½œ"ç­‰å†…éƒ¨è¿‡ç¨‹
5. å›ç­”è¯­æ°”å‹å¥½ï¼Œä½¿ç”¨ä¸­æ–‡ï¼Œå¹¶ç»™å‡ºè‡ªå·±çš„ä¸€äº›æ•°æ®åˆ†æå»ºè®®
6. å¦‚æœæœ‰ç›¸å…³çŸ¥è¯†å‚è€ƒï¼Œè¯·éµå¾ªå…¶ä¸­çš„è§„åˆ™å’Œå»ºè®®

## é‡è¦ï¼šå®Œæˆåå¿…é¡»æ€»ç»“
å½“ä½ å®Œæˆç”¨æˆ·è¯·æ±‚çš„æ‰€æœ‰æ“ä½œåï¼Œ**å¿…é¡»**ç»™å‡ºç®€æ´çš„å®Œæˆæ€»ç»“ï¼ŒåŒ…æ‹¬ï¼š
- å·²å®Œæˆçš„æ“ä½œæ¦‚è¿°
- å…³é”®æ•°æ®ç»“æœï¼ˆå¦‚ç»Ÿè®¡æ±‡æ€»å€¼ï¼‰
- æç¤ºç”¨æˆ·å¯ä»¥ç‚¹å‡»é¡µé¢å·¦ä¾§çš„"ä¸‹è½½æ–‡ä»¶"æŒ‰é’®è·å–ä¿®æ”¹åçš„æ–‡ä»¶
"""


# æ˜¯å¦å¯ç”¨ Skills åŠ¨æ€è·¯ç”±ï¼ˆå¯é€šè¿‡é…ç½®æ§åˆ¶ï¼‰
# è®¾ä¸º True å¯ç”¨æŠ€èƒ½æ‰«æå’ŒåŒ¹é…æ—¥å¿—
ENABLE_SKILL_ROUTING = True


def create_llm_for_model(model_name: str):
    """æ ¹æ®æ¨¡å‹åç§°åˆ›å»º LLM å®ä¾‹"""
    config = get_config()
    provider = config.model.get_active_provider()

    # æ ¹æ®æ¨¡å‹åç§°è‡ªåŠ¨é€‰æ‹© provider ç±»å‹
    # gemini/gpt æ¨¡å‹ä½¿ç”¨ OpenAI å…¼å®¹æ¥å£ï¼Œclaude æ¨¡å‹ä½¿ç”¨ Anthropic æ¥å£
    use_openai = model_name.startswith(("gemini", "gpt", "deepseek", "qwen", "glm"))

    # å¤„ç† base_url
    base_url = provider.base_url or ""

    if use_openai:
        # OpenAI æ ¼å¼éœ€è¦ç¡®ä¿æœ‰ /v1 åç¼€
        openai_base_url = base_url.rstrip('/')
        if not openai_base_url.endswith('/v1'):
            openai_base_url = openai_base_url + '/v1'
        return ChatOpenAI(
            model=model_name,
            api_key=provider.api_key,
            base_url=openai_base_url,
            temperature=provider.temperature,
            max_tokens=provider.max_tokens,
        )
    elif provider.provider == "anthropic" or model_name.startswith("claude"):
        # Anthropic æ ¼å¼éœ€è¦å»æ‰æœ«å°¾çš„æ–œæ 
        return ChatAnthropic(
            model=model_name,
            api_key=provider.api_key,
            base_url=base_url.rstrip('/') if base_url else None,
            temperature=provider.temperature,
            max_tokens=provider.max_tokens,
        )
    else:
        # é»˜è®¤ä½¿ç”¨ OpenAI å…¼å®¹æ¥å£
        return ChatOpenAI(
            model=model_name,
            api_key=provider.api_key,
            base_url=base_url if base_url else None,
            temperature=provider.temperature,
            max_tokens=provider.max_tokens,
        )


def get_model_list() -> List[str]:
    """è·å–æŒ‰ä¼˜å…ˆçº§æ’åºçš„æ¨¡å‹åˆ—è¡¨ï¼ˆä¸»æ¨¡å‹ + é™çº§æ¨¡å‹ï¼‰"""
    config = get_config()
    provider = config.model.get_active_provider()

    models = [provider.model_name]
    if provider.fallback_models:
        models.extend(provider.fallback_models)

    return models


def get_llm():
    """è·å– LLM å®ä¾‹ï¼ˆä½¿ç”¨ä¸»æ¨¡å‹ï¼‰"""
    config = get_config()
    provider = config.model.get_active_provider()
    return create_llm_for_model(provider.model_name)


async def stream_chat(message: str, history: list = None) -> AsyncGenerator[Dict[str, Any], None]:
    """æ‰§è¡Œå¯¹è¯ - ä½¿ç”¨ LangChain ReAct Agentï¼ˆæ”¯æŒæ¨¡å‹å®¹é”™ï¼‰

    Args:
        message: å½“å‰ç”¨æˆ·æ¶ˆæ¯
        history: å†å²å¯¹è¯åˆ—è¡¨ï¼Œæ¯é¡¹ä¸º {"role": "user"|"assistant", "content": "..."}

    Yields:
        çŠ¶æ€äº‹ä»¶ç±»å‹:
        - {"type": "status", "status": "processing"} - å¼€å§‹å¤„ç†ï¼Œå‰ç«¯åº”ç¦ç”¨è¾“å…¥
        - {"type": "status", "status": "idle"} - å¤„ç†å®Œæˆï¼Œå‰ç«¯å¯æ¢å¤è¾“å…¥
        - {"type": "thinking", "content": "..."} - æ€è€ƒè¿‡ç¨‹
        - {"type": "thinking_done"} - æ€è€ƒå®Œæˆ
        - {"type": "tool_call", "name": "...", "args": {...}} - å·¥å…·è°ƒç”¨
        - {"type": "tool_result", "name": "...", "result": {...}} - å·¥å…·ç»“æœ
        - {"type": "token", "content": "..."} - æµå¼è¾“å‡º token
        - {"type": "done", "content": "..."} - å›ç­”å®Œæˆ
        - {"type": "error", "content": "..."} - é”™è¯¯ä¿¡æ¯
    """
    global _current_model
    loader = get_loader()

    # å¼€å§‹å¤„ç† - é€šçŸ¥å‰ç«¯ç¦ç”¨è¾“å…¥
    yield {"type": "status", "status": "processing"}

    if not loader.is_loaded:
        yield {"type": "error", "content": "è¯·å…ˆä¸Šä¼  Excel æ–‡ä»¶"}
        yield {"type": "status", "status": "idle"}
        return

    # è·å–æ¨¡å‹åˆ—è¡¨ï¼ˆä¸»æ¨¡å‹ + é™çº§æ¨¡å‹ï¼‰
    model_list = get_model_list()
    last_error = None

    for model_idx, model_name in enumerate(model_list):
        try:
            _current_model = model_name

            # å¦‚æœä¸æ˜¯ç¬¬ä¸€ä¸ªæ¨¡å‹ï¼Œè¯´æ˜æ˜¯é™çº§
            if model_idx > 0:
                print(f"[æ¨¡å‹å®¹é”™] åˆ‡æ¢åˆ°é™çº§æ¨¡å‹: {model_name}")
                yield {"type": "thinking", "content": f"æ¨¡å‹è´Ÿè½½è¿‡é«˜ï¼Œåˆ‡æ¢åˆ° {model_name}..."}

            # æ‰§è¡Œå¯¹è¯
            async for event in _do_stream_chat(message, history, model_name):
                yield event

            # æˆåŠŸå®Œæˆï¼Œå‘é€ idle çŠ¶æ€åé€€å‡º
            yield {"type": "status", "status": "idle"}
            return

        except Exception as e:
            error_str = str(e)
            last_error = e

            # æ£€æŸ¥æ˜¯å¦æ˜¯å¯é‡è¯•çš„é”™è¯¯ï¼ˆæ¨¡å‹è´Ÿè½½ã€è¿æ¥é—®é¢˜ã€ç©ºå“åº”ç­‰ï¼‰
            is_retryable_error = (
                "500" in error_str or
                "è´Ÿè½½" in error_str or
                "overload" in error_str.lower() or
                "capacity" in error_str.lower() or
                "rate" in error_str.lower() or
                "no generations" in error_str.lower() or  # Gemini ç©ºæµå“åº”
                "empty" in error_str.lower() or
                "timeout" in error_str.lower() or
                "connection" in error_str.lower()
            )

            if is_retryable_error and model_idx < len(model_list) - 1:
                # è¿˜æœ‰é™çº§æ¨¡å‹å¯ç”¨ï¼Œç»§ç»­å°è¯•
                print(f"[æ¨¡å‹å®¹é”™] {model_name} ä¸å¯ç”¨: {error_str[:100]}")
                continue
            else:
                # æ²¡æœ‰æ›´å¤šæ¨¡å‹æˆ–ä¸æ˜¯è´Ÿè½½é”™è¯¯ï¼ŒæŠ›å‡ºå¼‚å¸¸
                import traceback
                traceback.print_exc()
                yield {"type": "thinking_done"}
                yield {"type": "error", "content": f"å¤„ç†å‡ºé”™: {str(e)}"}
                yield {"type": "status", "status": "idle"}
                return

    # æ‰€æœ‰æ¨¡å‹éƒ½å¤±è´¥
    yield {"type": "thinking_done"}
    yield {"type": "error", "content": f"æ‰€æœ‰æ¨¡å‹å‡ä¸å¯ç”¨ï¼Œæœ€åé”™è¯¯: {str(last_error)}"}
    yield {"type": "status", "status": "idle"}


async def _do_stream_chat(message: str, history: list, model_name: str) -> AsyncGenerator[Dict[str, Any], None]:
    """æ‰§è¡Œå•ä¸ªæ¨¡å‹çš„å¯¹è¯æµç¨‹"""
    loader = get_loader()

    excel_summary = loader.get_summary()
    llm = create_llm_for_model(model_name)

    # ä¸»å¯¹è¯å¼€å§‹
    yield {"type": "thinking", "content": f"æ­£åœ¨ä½¿ç”¨ {model_name} è§„åˆ’è§£ç­”..."}

    # æ£€ç´¢ç›¸å…³çŸ¥è¯†
    knowledge_context = "æš‚æ— ç›¸å…³çŸ¥è¯†å‚è€ƒã€‚"
    kb = get_knowledge_base()
    if kb:
        try:
            stats = kb.get_stats()
            print(f"[çŸ¥è¯†åº“] çŠ¶æ€: {stats['total_entries']} æ¡çŸ¥è¯†")
            relevant_knowledge = kb.search(query=message)
            print(f"[çŸ¥è¯†åº“] æ£€ç´¢åˆ° {len(relevant_knowledge)} æ¡ç›¸å…³çŸ¥è¯†")
            if relevant_knowledge:
                knowledge_context = format_knowledge_context(relevant_knowledge)
                yield {"type": "thinking", "content": f"æ‰¾åˆ° {len(relevant_knowledge)} æ¡ç›¸å…³çŸ¥è¯†å‚è€ƒ..."}
        except Exception as e:
            print(f"[çŸ¥è¯†åº“æ£€ç´¢] è­¦å‘Š: {e}")
            import traceback
            traceback.print_exc()
    else:
        print("[çŸ¥è¯†åº“] æœªå¯ç”¨æˆ–åˆå§‹åŒ–å¤±è´¥")

    # Skills åŠ¨æ€è·¯ç”±ï¼ˆä½¿ç”¨æ–°çš„ SkillLoaderï¼‰
    skills_context = "æ‰€æœ‰æ•°æ®åˆ†æå·¥å…·å‡å¯ç”¨ã€‚"
    active_tools = ALL_TOOLS  # é»˜è®¤ä½¿ç”¨æ‰€æœ‰å·¥å…·

    if ENABLE_SKILL_ROUTING:
        try:
            skill_loader = get_skill_loader()

            # è¾“å‡ºæŠ€èƒ½åˆ—è¡¨ï¼ˆç”¨äºç³»ç»Ÿæç¤ºï¼ŒèŠ‚çœ tokenï¼‰
            skill_list_prompt = skill_loader.get_skill_list_for_prompt()
            summary = skill_loader.get_skills_summary()
            print(f"\n[Skills] å¯ç”¨æŠ€èƒ½: {summary['total']} ä¸ª (æ ¸å¿ƒ: {summary['core']}, æŒ‰éœ€: {summary['on_demand']}, ç³»ç»Ÿ: {summary['system']})")

            # æ¿€æ´»ä¸æŸ¥è¯¢ç›¸å…³çš„æŠ€èƒ½ï¼ˆä¼šè¾“å‡ºè¯¦ç»†åŒ¹é…æ—¥å¿—ï¼‰
            activated_skills = skill_loader.activate_skills_for_query(message, top_k=5, threshold=0.25)

            if activated_skills:
                active_tools = skill_loader.get_active_tools()
                skill_names = skill_loader.get_active_skill_names()

                # ä½¿ç”¨ç´§å‡‘çš„æŠ€èƒ½åˆ—è¡¨æ ¼å¼ï¼ˆèŠ‚çœ tokenï¼‰
                skills_context = f"å·²æ¿€æ´»æŠ€èƒ½: {', '.join(skill_names)}\n"
                skills_context += skill_loader.get_system_prompt_additions()

                yield {"type": "thinking", "content": f"æ¿€æ´»æŠ€èƒ½: {', '.join(skill_names)}"}

                # è¾“å‡ºæ¿€æ´»ç»“æœæ‘˜è¦
                print(f"\n[Skills] âœ… æœ€ç»ˆæ¿€æ´»: {skill_names}")
                print(f"[Skills] ğŸ”§ å¯ç”¨å·¥å…·æ•°: {len(active_tools)}")
        except Exception as e:
            print(f"[Skills] âŒ è·¯ç”±å¤±è´¥ï¼Œä½¿ç”¨å…¨éƒ¨å·¥å…·: {e}")
            import traceback
            traceback.print_exc()
            active_tools = ALL_TOOLS
    else:
        # æœªå¯ç”¨æŠ€èƒ½è·¯ç”±æ—¶ï¼Œä¹Ÿè¾“å‡ºä¸€æ¡æ—¥å¿—
        print(f"\n[Skills] â¸ï¸  æŠ€èƒ½è·¯ç”±æœªå¯ç”¨ï¼Œä½¿ç”¨å…¨éƒ¨ {len(ALL_TOOLS)} ä¸ªå·¥å…·")

    # æ„å»ºç³»ç»Ÿæç¤º
    system_prompt = SYSTEM_PROMPT.format(
        excel_summary=excel_summary,
        knowledge_context=knowledge_context,
        skills_context=skills_context
    )

    # è·å–å½“å‰æ´»è·ƒè¡¨ä¿¡æ¯
    active_table_info = loader.get_active_table_info()
    current_table_name = active_table_info.filename if active_table_info else "æœªçŸ¥è¡¨"

    # åˆ›å»º ReAct Agentï¼ˆä½¿ç”¨åŠ¨æ€æ¿€æ´»çš„å·¥å…·ï¼‰
    agent = create_react_agent(llm, active_tools)

    # æ„å»ºæ¶ˆæ¯ - åŒ…å«å†å²å¯¹è¯
    current_message = f"[å½“å‰æ“ä½œè¡¨: {current_table_name}] {message}"
    messages = [SystemMessage(content=system_prompt)]

    # æ·»åŠ å†å²å¯¹è¯
    if history:
        from langchain_core.messages import AIMessage
        for msg in history:
            if msg.get("role") == "user":
                messages.append(HumanMessage(content=msg.get("content", "")))
            elif msg.get("role") == "assistant":
                messages.append(AIMessage(content=msg.get("content", "")))

    # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
    messages.append(HumanMessage(content=current_message))

    # ä½¿ç”¨ stream_mode="messages" è·å–çœŸæ­£çš„æµå¼è¾“å‡º
    thinking_content = ""
    final_content = ""
    tool_call_yielded = False
    thinking_done_sent = False

    # ç´¯ç§¯å·¥å…·è°ƒç”¨ä¿¡æ¯
    tool_names_by_id = {}  # id -> name
    args_by_index = {}  # index -> args_str
    tool_call_order = []  # è®°å½•å·¥å…·è°ƒç”¨çš„é¡ºåº [(id, index), ...]
    yielded_tool_ids = set()

    async for chunk in agent.astream(
        {"messages": messages},
        stream_mode="messages",
        config={"recursion_limit": 50}
    ):
        # chunk æ˜¯ä¸€ä¸ª tuple: (message, metadata)
        if isinstance(chunk, tuple) and len(chunk) >= 2:
            msg, metadata = chunk[0], chunk[1]

            # å¤„ç† AIMessageChunk (LLM è¾“å‡º)
            if isinstance(msg, AIMessageChunk):
                content = msg.content if hasattr(msg, 'content') else ""
                # å¤„ç† content å¯èƒ½æ˜¯åˆ—è¡¨çš„æƒ…å†µï¼ˆAnthropic æ ¼å¼ï¼‰
                if isinstance(content, list):
                    text_parts = []
                    for block in content:
                        if isinstance(block, dict) and block.get("type") == "text":
                            text_parts.append(block.get("text", ""))
                        elif isinstance(block, str):
                            text_parts.append(block)
                    content = "".join(text_parts)
                tool_call_chunks = getattr(msg, 'tool_call_chunks', [])

                # ç´¯ç§¯å·¥å…·è°ƒç”¨çš„ chunks
                if tool_call_chunks:
                    for tcc in tool_call_chunks:
                        tc_id = tcc.get("id")
                        tc_name = tcc.get("name", "")
                        tc_args = tcc.get("args", "")
                        tc_index = tcc.get("index", 0)

                        # å¦‚æœæœ‰æ–°çš„å·¥å…· id å‡ºç°ï¼ˆå¸¦ nameï¼‰ï¼Œè®°å½•ä¸‹æ¥
                        if tc_id and tc_name:
                            if tc_id not in tool_names_by_id:
                                tool_call_order.append((tc_id, tc_index))
                                if not thinking_done_sent:
                                    yield {"type": "thinking_done"}
                                    thinking_done_sent = True
                            tool_names_by_id[tc_id] = tc_name

                        # ç´¯ç§¯ argsï¼ˆæŒ‰ indexï¼‰
                        if tc_args:
                            if tc_index not in args_by_index:
                                args_by_index[tc_index] = ""
                            args_by_index[tc_index] += tc_args

                # å¦‚æœæœ‰æ–‡æœ¬å†…å®¹
                if content:
                    if tool_call_yielded:
                        # å·¥å…·è°ƒç”¨åçš„å†…å®¹æ˜¯æœ€ç»ˆå›ç­”
                        final_content += content
                        yield {"type": "token", "content": content}
                    else:
                        # å·¥å…·è°ƒç”¨å‰çš„å†…å®¹æ˜¯æ€è€ƒè¿‡ç¨‹
                        thinking_content += content
                        yield {"type": "thinking", "content": thinking_content}

            # å¤„ç† ToolMessage (å·¥å…·ç»“æœ)
            elif isinstance(msg, ToolMessage):
                tool_call_id = msg.tool_call_id if hasattr(msg, 'tool_call_id') else None
                tool_name = msg.name if hasattr(msg, 'name') else "tool"
                tool_content = msg.content

                # åœ¨å‘é€ tool_result ä¹‹å‰ï¼Œå…ˆå‘é€å¯¹åº”çš„ tool_call
                if tool_call_id and tool_call_id not in yielded_tool_ids:
                    yielded_tool_ids.add(tool_call_id)
                    tool_call_yielded = True

                    # æ‰¾åˆ°è¿™ä¸ªå·¥å…·è°ƒç”¨çš„ index
                    tc_index = 0
                    for (tid, idx) in tool_call_order:
                        if tid == tool_call_id:
                            tc_index = idx
                            break

                    # ä» args_by_index è·å– args
                    args_str = args_by_index.get(tc_index, "{}")

                    # è§£æ args
                    try:
                        args = json.loads(args_str)
                    except json.JSONDecodeError:
                        try:
                            last_brace = args_str.rfind('{"')
                            if last_brace >= 0:
                                args = json.loads(args_str[last_brace:])
                            else:
                                args = {"raw": args_str}
                        except:
                            args = {"raw": args_str}

                    # è·å–å·¥å…·åç§°
                    tc_name = tool_names_by_id.get(tool_call_id, tool_name)

                    yield {
                        "type": "tool_call",
                        "name": tc_name,
                        "args": args,
                    }

                    # æ¸…é™¤å·²å¤„ç†çš„ args
                    if tc_index in args_by_index:
                        del args_by_index[tc_index]

                # å‘é€å·¥å…·ç»“æœ
                try:
                    result = json.loads(tool_content)
                except:
                    result = {"result": tool_content}

                yield {
                    "type": "tool_result",
                    "name": tool_name,
                    "result": result,
                }

    # æµå¼ç»“æŸ
    if not tool_call_yielded and thinking_content:
        # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œthinking_content å°±æ˜¯æœ€ç»ˆå›ç­”
        if not thinking_done_sent:
            yield {"type": "thinking_done"}
        yield {"type": "clear_thinking"}
        yield {"type": "token", "content": thinking_content}
        yield {"type": "done", "content": thinking_content}
    elif final_content:
        yield {"type": "done", "content": final_content}
    elif tool_call_yielded:
        # å·¥å…·è°ƒç”¨åæ²¡æœ‰ç”Ÿæˆæ€»ç»“ï¼Œæ·»åŠ åå¤‡æç¤º
        fallback_message = "\n\nâœ… **æ“ä½œå·²å®Œæˆ**\n\nå¦‚éœ€è·å–ä¿®æ”¹åçš„æ–‡ä»¶ï¼Œè¯·ç‚¹å‡»é¡µé¢å·¦ä¾§çš„ã€Œä¸‹è½½æ–‡ä»¶ã€æŒ‰é’®ã€‚"
        yield {"type": "token", "content": fallback_message}
        yield {"type": "done", "content": fallback_message}
    else:
        yield {"type": "done", "content": ""}
